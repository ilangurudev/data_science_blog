{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For my Machine Learing class at A&M, I implemented common ML algorithms from the ground up. It's amazing how easy it is to implement an ML technique in python. I thought that I'd share my implementations in a series of short blog posts. (I am new to python, so please cut me some slack.) In my opinion, there are three main advantages to this endeavour:\n",
    "\n",
    "- The understanding we get when we implement something from scratch is unmatched. A lot of the times, we think we understand a concept but when we implement it, we get a slew of doubts, thereby forcing us to really get the details right. \n",
    "- It helps us appreciate the design of libraries better and understand why things are designed a certain way. It helps truly get the API of a library with enduring success like scikit-learn.  \n",
    "- The satisfaction when we see the identical results convinces, that these libraries do not do something like the algorithm we study but actually implement the very algorithm we study, only better. \n",
    "\n",
    "\n",
    "# Techniques and Design\n",
    "\n",
    "In this blog, I am sharing the code for \n",
    "\n",
    "1. A small module to scale training and testing dataset features, the right way.\n",
    "2. A K Nearest Neighbour classifier\n",
    "\n",
    "Typically the features are converted to the same scale before we use a distance based classifier like KNN to classify. This is to ensure that all features are given equal importance. The standardizing can be done by min-max scaling or z-scaling. \n",
    "\n",
    "In a package like scikit learn, each step is modularized. We can then aggregate multiple steps using pipelines (See last section). This means that if we do not scale features manually, the algorithm executes KNN without translating all features to the same scale. I, however, decided to make scaling part of my KNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (Libraries and Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this series of posts concentrates on the implementations, I won't delve into the data and results per se. I'll just briefly explain the dataset and the classification task for demonstration purposes. The dataset shows the response - area (in square feet) affected by volcanos and different features corresponding to it. We can change this to a classification problem by predicting whether the area was affected by volcanoes or not (area > 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X  Y  month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area\n",
      "0  7  5      3    5  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0\n",
      "1  7  4     10    2  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0\n",
      "2  7  4     10    6  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0\n",
      "3  8  6      3    5  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0\n",
      "4  8  6      3    7  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "df_train_org = pd.read_csv(\"../data/ml_scratch_1_train.csv\")\n",
    "df_test_org = pd.read_csv(\"../data/ml_scratch_1_test.csv\")\n",
    "print(df_train_org.head())\n",
    "\n",
    "tr_X = df_train_org.loc[:, df_train_org.columns != 'area']\n",
    "ts_X = df_test_org.loc[:, tr_X.columns]\n",
    "tr_y = (df_train_org['area'] > 0)*1\n",
    "ts_y = (df_test_org['area'] > 0)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a basic template for a standardizer that can do both Z-scaling and min-max scaling. The advantage of having such a standardizer is that we can store the mean, sd, min and max values of the training set and use it when we standardize the test set. This is very important as we cannot standardize the test set using the test set summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardize():\n",
    "    \n",
    "\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    @property\n",
    "    def normalize_mappings(self):\n",
    "        mu = self.X.mean()\n",
    "        sd = self.X.std()\n",
    "        return {'mu': mu, 'sd': sd}\n",
    "\n",
    "    @property\n",
    "    def min_max_mappings(self):\n",
    "        mins = self.X.min()\n",
    "        maxs = self.X.max()\n",
    "        return {'mins': mins, 'maxs': maxs}\n",
    "\n",
    "    def standardize(self, method=\"normalize\", X=None):\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "        if method == \"normalize\":\n",
    "            X = (X - self.normalize_mappings['mu'])/self.normalize_mappings['sd']\n",
    "        elif method == \"min_max\":\n",
    "            X = ((X - self.min_max_mappings['mins'])\n",
    "                 / (self.min_max_mappings['maxs'] - self.min_max_mappings['mins']))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a KNN classifier\n",
    "\n",
    "We create a KNN classifier using 3 parameters:\n",
    "\n",
    "1. Number of nearest neighbours, k. \n",
    "2. When we calculate the distance, the norm to be used, norm. \n",
    "3. The standardization to be used, standardization\n",
    "\n",
    "## Fitting a classifier\n",
    "\n",
    "The fit method for a KNN classifier is fairly minimal. We\n",
    "\n",
    "1. Create (and store) a standarizer using the training data passed. \n",
    "2. Standardize the training data using the stored standardizer and store the standardized training data.\n",
    "3. Store the labels\n",
    "\n",
    "## Prediction\n",
    "\n",
    "The predict method is the most important method because this is where we need to do most work. The steps are listed out:\n",
    "\n",
    "1. First, we standardize the test data using the standardizer we stored during the fit procedure. This ensures, test data is standardized according to training data mappings. \n",
    "2. Next, we get the distances of test data points from all training data points (**dist_matrix**). This distance matrix has dimensions n_test x n_train. We use a scipy function called distance_matrix for this and we pass the norm to this function. I tried implementing my own distance matrix and though, I got the right answer, the process was painfully slow. \n",
    "3. Once we get the distance matrix, for each observation in test data, we find the indices of the k closest training observations (**get_nearest_neighbour_indices**). \n",
    "4. Since we have the labels for all indices, for each test data observtion, we count the labels among the k closest neighbours (**count_votes**). \n",
    "5. Once we count the votes for each observation, we just have to pick the class with the most votes (**tag_winner**). In case of a tie, ideally a class should be picked at random but in the current implementaion, the first class returned by collections.Counter is returned. For KNNs, it anyway better to give an odd k, so I didn't spend too much time on it. \n",
    "6. Thus for each test point, a class is returned. \n",
    "\n",
    "## Score\n",
    "\n",
    "This is a simple function to get the accuracy of the predicted values. \n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Knn():\n",
    "\n",
    "    def __init__(self, \n",
    "                 k=3,\n",
    "                 norm=2.0 ,\n",
    "                 standardization=\"normalize\"):\n",
    "\n",
    "        self.k = k\n",
    "        self.norm = norm\n",
    "        self.standardization = standardization\n",
    "  \n",
    "\n",
    "    def fit(self, tr_X, tr_y):\n",
    "        \"\"\"Pass training data as X and y \"\"\"\n",
    "        self.standardizer = Standardize(X=tr_X)\n",
    "        self.tr_X = self.standardizer.standardize(method=self.standardization)\n",
    "        self.tr_y = tr_y\n",
    "\n",
    "\n",
    "    def predict(self, ts_X):\n",
    "        ts_X = self.standardizer.standardize(\n",
    "            method=self.standardization, X=ts_X)\n",
    "        d = self._dist_matrix(ts_X)\n",
    "        neighbours = self._get_nearest_neighbour_indices(d)\n",
    "        counts = self._count_votes(neighbours)\n",
    "        return self._tag_winner(counts)\n",
    "\n",
    "\n",
    "    def _dist_matrix(self, ts_X):\n",
    "        dists = scipy.spatial.distance_matrix(ts_X, self.tr_X, p=self.norm)\n",
    "        dists = np.vstack(dists)\n",
    "        return dists\n",
    "\n",
    "\n",
    "    def _get_nearest_neighbour_indices(self, dist_matrix):\n",
    "        return np.argpartition(dist_matrix, kth=self.k, axis=1)[:, :self.k]\n",
    "\n",
    "\n",
    "    def _count_votes(self, indice_matrix):\n",
    "        return np.apply_along_axis(lambda x: collections.Counter(self.tr_y.iloc[x]), 1, indice_matrix)\n",
    "\n",
    "\n",
    "    def _tag_winner(self, vote_mat):\n",
    "        count_max = np.vectorize(lambda x: x.most_common(1)[0][0])\n",
    "        return count_max(vote_mat)\n",
    "\n",
    "\n",
    "    def score(self, ts_X, ts_y):\n",
    "        return (ts_y == self.predict(ts_X)).mean()\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f'Knn(k={self.k}, norm={self.norm}, standardization=\"{self.standardization}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470588235294118"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn = Knn(k=13, norm=2, standardization=\"min_max\")\n",
    "model_knn.fit(tr_X, tr_y)\n",
    "model_knn.score(ts_X, ts_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying the results using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470588235294118"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_std_knn = pipeline.Pipeline([('scaling', MinMaxScaler()),\n",
    "                       ('knn', KNeighborsClassifier(n_neighbors=13))])\n",
    "model_std_knn.fit(tr_X, tr_y)\n",
    "model_std_knn.score(ts_X, ts_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results match!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
