---
title: What your texts say about you
author: Gurudev Ilangovan
date: '2018-10-03'
slug: whatsapp_text_analysis
categories: []
tags: [text, analysis, tidyverse]
showonlyimage: false
draft: false
image: "analyses/img/whatsapp.jpg"
weight: 1
type: "post"
description: "Using simple text mining techniques to understand how we text..."
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#processing">Processing</a><ul>
<li><a href="#patterns">Patterns</a></li>
<li><a href="#parsing">Parsing</a></li>
</ul></li>
<li><a href="#analysis">Analysis</a><ul>
<li><a href="#pet-words">Pet words</a><ul>
<li><a href="#overall-word-usage">Overall word usage</a></li>
<li><a href="#word-usage-by-member">Word usage by member</a></li>
<li><a href="#word-usage-made-better">Word usage made better</a></li>
</ul></li>
<li><a href="#section">⌚ 4⃣ 😂😋😍</a><ul>
<li><a href="#popular-emojis-and-chronic-roflitis">Popular emojis and chronic roflitis</a></li>
<li><a href="#lets-put-a-smiley-on-that-face">Let’s put a smiley on that face</a></li>
<li><a href="#emoji-to-word-ratio">Emoji to word ratio</a></li>
</ul></li>
<li><a href="#getting-sentient-about-sentiments">Getting sentient about sentiments</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<!-- I was reading Tidy Text Mining by David Robinson and Julia Silge where they walk us through text mining Jane Austen's work. I wanted to test out the same idea for analyzing our texting patterns. How do we chat? Is it possible to tease out trends from our pinging habits? -->
<blockquote>
<p>Without data, you’re just an another person with an opinion
- W. Edwards Deming</p>
</blockquote>
<p>We all have intuitions, questions or even hypotheses if you will about people’s texting patterns. Tom uses a lot of emojis, Daisy is a cynic and Harry swears a lot. But <em>unless we can substantiate our theories with data our guesses will remain speculations however educated</em>.</p>
<p>I had a few questions. What are people’s most used words? What do these words tell about us? What are the most popular emojis? Are people positive or negative in general when they speak? To answer such questions I decided to look at chats and the place where I have the most chats is Whatsapp. So I decided to take one group chat from whatsapp for analysis. Obviously, blogging about chats with other people is tricky, because… it involves chats with other people. Apart from getting the said group’s members’ consent, I <strong>anonymized</strong> their names and I have taken utmost care not to publish the chats themselves as far as possible (Apart from a preview of the first 10 ones).</p>
</div>
<div id="processing" class="section level2">
<h2>Processing</h2>
<p>If you are interested only in the results, please ignore the code blocks wherever you see. Also, skip this section and head over to the <a href="#analysis">analysis</a> section right away.</p>
<p>If any of you want to repeat it for your personal chats, this is the method I followed:</p>
<ol style="list-style-type: decimal">
<li>Go to the chat window that you want to analyze on whatsapp.</li>
<li>Go to options and there will be an option to export the chat as email.</li>
<li>Send it to your email and download the chat text file</li>
</ol>
<p>I am using whatsapp from an Android phone and I followed the above steps to get the chat text file. Naturally, I searched for preexisting R packages to parse the text file. There were a few packages but they were outdated and incompatible with the current format of the export file. So I wrote my own parser.</p>
<div id="patterns" class="section level3">
<h3>Patterns</h3>
<p>I am basically constructing pattern matchers using regular expressions but I am using an excellent package called rebus that generates the regex patterns using human readable functions. I am pretty sure the patterns I have generated won’t cover all the possibilities and so might not work for you but with a little tweaking, it can easily be made to work.</p>
<pre class="r"><code>pacman::p_load(tidyverse, rebus, lubridate, tidytext, magrittr,
               sentimentr, magrittr, plotly, rtweet, ggthemes)

df_emojis &lt;- rtweet::emojis

pattern_emojis &lt;- df_emojis %&gt;% pull(code) %&gt;%  literal() %&gt;% or1()

pattern_new_message &lt;- 
  NEWLINE %R% one_or_more(DGT) %R% &quot;/&quot; %R% one_or_more(DGT) %R% &quot;/&quot; %R% 
  one_or_more(DGT) %R% &quot;,&quot; %R%
  SPACE %R% one_or_more(DGT) %R% &quot;:&quot; %R% one_or_more(DGT) %R% 
  rebus::or(&quot; AM&quot;, &quot; PM&quot;) %R% &quot; - &quot;

pattern_phone &lt;- 
  literal(&quot;+&quot;) %R% 
  one_or_more(DGT) %R% 
  SPACE %R%
  one_or_more(DGT) %R%
  SPACE %R%
  one_or_more(DGT)

pattern_name &lt;- 
  one_or_more(ALPHA) %R%
  zero_or_more(SPACE) %R%
  zero_or_more(ALPHA) %R%
  zero_or_more(SPACE) %R%
  zero_or_more(ALPHA)</code></pre>
</div>
<div id="parsing" class="section level3">
<h3>Parsing</h3>
<p>After generating the patterns, I executed the below code block to parse the file. (Actually, I ran this and a few more commands to anonymize people. I also removed a list of words from the analysis in the interest of anonymity.)</p>
<pre class="r"><code># input file
txt_chat &lt;- read_file(&quot;~/projects/sandbox/data/xxx.txt&quot;)

txt_split &lt;- 
  txt_chat %&gt;% 
  str_split(pattern_new_message) %&gt;% 
  pluck(1)

txt_split &lt;- txt_split[2:length(txt_split)]

txt_new_message_pattern &lt;- 
  txt_chat %&gt;% 
  str_extract_all(pattern_new_message) %&gt;% 
  pluck(1)


df_chats &lt;- 
  tibble(chat_time = txt_new_message_pattern,
         chat_text = txt_split) %&gt;%
  mutate(sender = 
           chat_text %&gt;% 
           str_extract(rebus::or(pattern_phone, pattern_name)),
         text = 
           chat_text %&gt;% 
           str_replace(literal(sender), &quot;&quot;) %&gt;% 
           str_replace(zero_or_more(literal(&quot;:&quot;)), &quot;&quot;) %&gt;% 
           str_replace(SPACE, &quot;&quot;) %&gt;% 
           str_trim(),
         time = mdy_hm(chat_time), 
         emoji = str_extract(text, one_or_more(pattern_emojis)),
         emoji_count = str_count(text, pattern_emojis),
         media = text %&gt;% str_detect(literal(&quot;&lt;Media omitted&gt;&quot;)),
         text = ifelse(media, &quot;&quot;, text)
         ) %&gt;%
  add_count(sender) %&gt;%
  filter(!(sender %&gt;% str_detect(DGT)),
         !(sender %&gt;% str_detect(or1(c(&quot;created&quot;, &quot;added&quot;, &quot;left&quot;, &quot;changed&quot;))))) %&gt;%
  mutate(id_chat = row_number()) %&gt;% 
  select(id_chat, time, sender, text, media,  emoji, emoji_count)</code></pre>
Let’s see how the table looks.
<div style="border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; ">
<table class="table table-striped table-hover table-responsive" style="width: auto !important; ">
<thead>
<tr>
<th style="text-align:right;">
id_chat
</th>
<th style="text-align:left;">
time
</th>
<th style="text-align:left;">
sender
</th>
<th style="text-align:left;">
text
</th>
<th style="text-align:left;">
media
</th>
<th style="text-align:left;">
emoji
</th>
<th style="text-align:right;">
emoji_count
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
2018-05-07 23:45:00
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:left;">
My life 😂😂😂😂😂
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
😂😂😂😂😂
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
2018-05-07 23:47:00
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:left;">
Sorry for starting on a morbid note 😂😂
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
😂😂
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
2018-05-07 23:47:00
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:left;">
It just came
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
2018-05-07 23:48:00
</td>
<td style="text-align:left;">
B
</td>
<td style="text-align:left;">
Wait
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
2018-05-07 23:48:00
</td>
<td style="text-align:left;">
B
</td>
<td style="text-align:left;">
One bad joke is coming up
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
2018-05-07 23:49:00
</td>
<td style="text-align:left;">
B
</td>
<td style="text-align:left;">
Poor network
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
2018-05-07 23:49:00
</td>
<td style="text-align:left;">
B
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
2018-05-07 23:50:00
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:left;">
😂😂😂😂😂
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
😂😂😂😂😂
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
2018-05-07 23:52:00
</td>
<td style="text-align:left;">
B
</td>
<td style="text-align:left;">
All pls to indicate that you exist by periodically posting said bad jokes. Nandri🙏🏼
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
🙏
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
2018-05-07 23:53:00
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:left;">
When guys say bad jokes they usually mean something like this
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
</div>
<p>Awesome. Now that we have data in a tidy format, let’s delve into the analysis.</p>
</div>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<div id="pet-words" class="section level3">
<h3>Pet words</h3>
<div id="overall-word-usage" class="section level4">
<h4>Overall word usage</h4>
<p>What words do the group members use a lot overall?</p>
<pre class="r"><code>df_words &lt;- 
  df_chats %&gt;% 
  unnest_tokens(word, text) %&gt;% 
  anti_join(stop_words, &quot;word&quot;) %&gt;% 
  filter(!word %in% var_ignore_words) 

df_words %&gt;% 
  add_count(word) %&gt;% 
  mutate(rank = dense_rank(desc(n))) %&gt;% 
  filter(rank &lt;= 15) %&gt;% 
  arrange(rank) %&gt;% 
  mutate(word = fct_inorder(word) %&gt;% fct_rev()) %&gt;% 
  ggplot(aes(word, fill = sender)) +
  geom_bar() +
  coord_flip() +
  theme_minimal() +
  scale_fill_tableau()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-6-1.png" width="100%" /></p>
<p>The members seem to <strong>agree</strong>. A lot.</p>
<p>If you’re wondering how words like “and” and “the” are not the most common, kudos. Those words are indeed the most common but I have removed such words (commonly called <em>“stop words”</em> in text mining) because they are not informative.</p>
</div>
<div id="word-usage-by-member" class="section level4">
<h4>Word usage by member</h4>
<p>And what words do each of the members use? Let’s look at the five most used words.</p>
<pre class="r"><code>df_words %&gt;% 
  count(sender, word, sort = T) %&gt;% 
  group_by(sender) %&gt;% 
  arrange(desc(n), .by_group = T) %&gt;% 
  slice(1:5) %&gt;% 
  ungroup() %&gt;% 
  mutate(word = word %&gt;% fct_inorder() %&gt;% fct_rev()) %&gt;% 
  ggplot(aes(word, n, fill = sender)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  facet_grid(sender ~ ., scales=&quot;free&quot;) +
  theme(strip.text.x = element_text(angle = 0)) +
  scale_fill_tableau()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-7-1.png" width="100%" /></p>
</div>
<div id="word-usage-made-better" class="section level4">
<h4>Word usage made better</h4>
<p>Well, most of the words are commonly used ones. Very predictable and not very informative. How do we find words that are important relative to each member? Enter <strong>tf-idf</strong>! Tf-idf(Term frequency - Inverse document frequency) is a concept used in information retrieval where in the importance of each term is measured relative to the commonly that term is used by others. Basically, if you are using a term a lot and that term is used a lot in general, the count does not accurately reflect who you are. It’s the words that you use a lot but others don’t that say more about you. That’s the basic intuition but the <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">Wikipedia</a> article is actually pretty accessible if you want to dig deeper.</p>
<pre class="r"><code>df_words %&gt;% 
  count(sender, word) %&gt;% 
  filter(n &gt; 15) %&gt;% 
  bind_tf_idf(word, sender, n) %&gt;% 
  group_by(sender) %&gt;% 
  arrange(desc(tf_idf)) %&gt;% 
  slice(1:5) %&gt;% 
  ungroup() %&gt;% 
  mutate(word = word %&gt;% fct_inorder() %&gt;% fct_rev()) %&gt;% 
  ggplot(aes(word, tf_idf, fill = sender)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  facet_grid(sender ~ ., scales=&quot;free&quot;) +
  theme(strip.text.x = element_text(angle = 0)) +
  scale_fill_tableau()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-8-1.png" width="100%" /></p>
<p>I feel very foggy personalities seem to emerge.</p>
<ul>
<li>A - playful?</li>
<li>B - philosophical - Just look at the words: life, duty, book, days…</li>
<li>C - jejune?</li>
</ul>
<p>Can actually look at more words but don’t want to give too much away. 😜</p>
</div>
</div>
<div id="section" class="section level3">
<h3>⌚ 4⃣ 😂😋😍</h3>
<p>In case you are wondering what the heading means, it’s “Time for emojis”. I promise I’ll try not to do that again. Anyway, can we repeat the same analysis that we did with words for emojis?</p>
<div id="popular-emojis-and-chronic-roflitis" class="section level4">
<h4>Popular emojis and chronic roflitis</h4>
<p>Most used emojis in group:</p>
<pre class="r"><code>df_emoji_split &lt;- 
  df_chats %&gt;% 
  group_by(sender) %&gt;% 
  summarise(emoji = emoji %&gt;% ifelse(is.na(.), &quot;&quot;, .) %&gt;% paste0(collapse = &quot;&quot;),
            emoji_count = sum(emoji_count, na.rm = T)) %&gt;% 
  mutate(emoji_split = 
           map(emoji, function(emo){
             emo &lt;- emo %&gt;% str_split(&quot;&quot;, simplify = T) %&gt;% .[1,]
             tibble(emoji = emo) %&gt;% 
             left_join(df_emojis, by = c(&quot;emoji&quot; = &quot;code&quot;))
           })) %&gt;% 
  select(sender, emoji_split) %&gt;% 
  unnest()

df_emoji_split %&gt;% 
  add_count(emoji, sort = T) %&gt;% 
  mutate(rank = dense_rank(desc(n))) %&gt;% 
  filter(rank &lt;= 10) %&gt;% 
  arrange(desc(n)) %&gt;% 
  mutate(emoji = fct_inorder(emoji) %&gt;% fct_rev()) %&gt;% 
  ggplot(aes(emoji, fill = sender)) +
  geom_bar() +
  coord_flip() +
  theme_minimal() +
  theme(axis.text = element_text(size = 20)) +
  scale_fill_tableau()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-9-1.png" width="100%" /></p>
<p>My personal hypothesis was right - A has a serious case of roflitis.</p>
</div>
<div id="lets-put-a-smiley-on-that-face" class="section level4">
<h4>Let’s put a smiley on that face</h4>
<p>Using the same concept of tf-idf we can now find out which emojis characterize people better than raw counts.</p>
<pre class="r"><code>df_emoji_split %&gt;% 
  count(sender, emoji,description, sort = T) %&gt;%
  bind_tf_idf(emoji, sender, n = n) %&gt;% 
  group_by(sender) %&gt;% 
  filter(tf_idf == max(tf_idf)) %&gt;% 
  arrange(sender) %&gt;%
  ungroup() %&gt;% 
  mutate(sender = sender %&gt;% fct_inorder() ) %&gt;% 
  ggplot(aes(sender, y = 10, label = emoji)) +
  geom_text(size = 30) +
  theme_minimal() +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_text(size = 40),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) </code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-10-1.png" width="100%" /></p>
</div>
<div id="emoji-to-word-ratio" class="section level4">
<h4>Emoji to word ratio</h4>
<pre class="r"><code>df_chats %&gt;% 
  unnest_tokens(word, text) %&gt;% 
  group_by(id_chat, sender) %&gt;% 
  summarise(word_count = n()) %&gt;% 
  left_join(
    df_chats %&gt;% 
    # filter(!is.na(emoji)) %&gt;% 
    select(id_chat, time, emoji_count),
    by = &quot;id_chat&quot;
  ) %&gt;% 
  ungroup() %&gt;%
  mutate(week = week(time)) %&gt;% 
  group_by(sender, week) %&gt;% 
  summarise(emoji_count_sum = sum(emoji_count),
            word_count_sum = sum(word_count),
            emoji_to_word = emoji_count_sum/word_count_sum) %&gt;% 
  ggplot(aes(week, emoji_to_word, col = sender)) +
  ylim(0, NA) +
  geom_smooth(se = F) +
  scale_color_tableau() +
  theme_minimal()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-11-1.png" width="100%" /></p>
<p>The plot could mean one of two things:</p>
<ul>
<li>Something genuinely funny or sad or emotional was being discussed in weeks 19 and 28.</li>
<li>A classic case of conformity or even, regression towards the mean? If one person uses a lot of emojis, other members tend to too. This seems more likely as there 3 lines follow the same trend from start to finish.</li>
</ul>
<p>Also is A’s steady decline in emoji usage an indication of realization of roflitis as was quickly pointed out in the group? I may never know.</p>
</div>
</div>
<div id="getting-sentient-about-sentiments" class="section level3">
<h3>Getting sentient about sentiments</h3>
<p>Finally let’s take a look at the chats from a sentiment perspective. <strong>Sentiment analysis</strong> is the process of analyzing a given corpus of text and then using some method to arrive at sentiment scores. Naively done, a score is assigned to a word and the scores of all words in a sentence are added. But a sentence like <em>“The movie was not good at all”</em> would still be <em>positively evaluated</em> by such a technique. So I am using a package called <a href="https://github.com/trinker/sentimentr"><code>sentimentr</code></a> which accounts for valence shifters (like ‘not’) and intensifiers (like ‘very’). For example,</p>
<pre><code>tibble(
  text = c(&quot;The movie was horrible&quot;,
            &quot;The movie was not good at all&quot;,
            &quot;The movie was okay&quot;,
            &quot;The movie was good&quot;,
            &quot;The movie was very impressive&quot;)) %&gt;% 
   mutate(num = row_number(),
          review = get_sentences(text)) %$% 
   sentiment_by(review, list(num, text)) %&gt;%
   sentimentr::highlight()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/sentiment.png" alt="sentiment_example" style="width:75%;"/></p>
<p>Now that we know what we can do, let’s calculate the sentiment scores for the chats.</p>
<pre class="r"><code>df_sentiment &lt;- 
  df_chats %&gt;% 
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_sent = get_sentences(sentence),
         id_sentence = row_number())


df_sentiment &lt;- 
  df_sentiment %$%
  sentiment_by(sentence_sent, list(id_sentence, sender)) %&gt;% 
  left_join(df_sentiment, c(&quot;id_sentence&quot;,&quot;sender&quot;)) %&gt;% 
  rename(sentiment = ave_sentiment) %&gt;% 
  as_tibble()</code></pre>
<p>The overall sentiment scores for the members.</p>
<pre class="r"><code>df_sentiment %&gt;% 
  filter(sentiment!= 0) %&gt;% 
  mutate(rating = ifelse(sentiment &gt; 0, &quot;postive&quot;,&quot;negative&quot;),
         sender = sender %&gt;% fct_rev(),
         week = week(time)) %&gt;% 
  group_by(sender, week, rating, ) %&gt;% 
  summarise(sentiment_sum = sum(sentiment),
            sentiment_mean = mean(sentiment)) %&gt;% 
  ungroup() %&gt;% 
  ggplot(aes(sender, sentiment_sum, fill = rating)) +
  geom_col() +
  theme_minimal() +
  coord_flip() +
  scale_fill_hue(l=40) </code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-13-1.png" width="100%" /></p>
<p>A and C have almost identical bars. B has a clear offset to the left. Perhaps my claim that B is a little philosophical might be true after all. Sometimes the more we think about life, less rosy it seems.</p>
<p>Also, this graph could be redrawn after accounting for the sentiments that emojis represent. Currently, <code>sentimentr</code> does not support emojis. However we could substitute the description of the emojis for the emojis themselves and then calculate the sentiments. With A’s roflitis, I suspect that this approach would make others seem too pessimistic in comparison. So, I didn’t go that route.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Admittedly, one group-chat tells only a small part of the story. If there were a way to get all whatsapp chats in a single step, we could do much richer analyses. For instance, chat behavior of the same person in different groups would be interesting to see. Similarly, it could be seen how the proportion of texts sent to each group or person varies across time. That could shed light into which bonds are becoming stronger and which ones are becoming more weaker. Sentiment analysis on all our texts can help us understand how our moods vary. So many possibilities…</p>
<p>Any comments, feedback or suggestions are, as people say, 😍😍😍.</p>
</div>
