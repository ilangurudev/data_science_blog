---
title: What your whatsapp texts say about you
author: Gurudev Ilangovan
date: '2018-10-03'
slug: whatsapp_text_analysis
categories: []
tags: [text, analysis, tidyverse]
showonlyimage: false
draft: false
image: "analyses/img/whatsapp.jpg"
weight: 1
type: "post"
description: "Using simple text ining techniques to understand how we text..."
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#processing">Processing</a><ul>
<li><a href="#patterns">Patterns</a></li>
<li><a href="#parsing">Parsing</a></li>
</ul></li>
<li><a href="#analysis">Analysis</a><ul>
<li><a href="#pet-words">Pet words</a></li>
<li><a href="#section">âŒš 4âƒ£ ğŸ˜‚ğŸ˜‹ğŸ˜</a></li>
<li><a href="#sentiment-sentience">Sentiment sentience</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<!-- I was reading Tidy Text Mining by David Robinson and Julia Silge where they walk us through text mining Jane Austen's work. I wanted to test out the same idea for analyzing our texting patterns. How do we chat? Is it possible to tease out trends from our pinging habits? -->
<blockquote>
<p>Without data, youâ€™re just an another person with an opinion
- W. Edwards Deming</p>
</blockquote>
<p>We all have intuitions, questions or even hypotheses if you will about peopleâ€™s texting patterns. Tom uses a lot of emojis, Daisy is a cynic and Harry swears a lot. But unless we can substantiate our theories with data our guesses will remain speculations however educated.</p>
<p>I had a few questions. What are peopleâ€™s most used words? Are people positive or negative in general when they speak? How do people use emojis? Do people</p>
<p>The place where I have the most chats is Whatsapp. So I decided to take one group chat from whatsapp for analysis. Obviosuly, blogging about chats with other people is tricky, becauseâ€¦ it involves chats with other people. Apart from getting the said groupâ€™s membersâ€™ consent, I anonymized their names and I have taken utmost care not to publish the chats themselves as far as possible (Apart from a preview of the first 10 ones).</p>
</div>
<div id="processing" class="section level2">
<h2>Processing</h2>
<p>If you are interested only in the results, please ignore the code blocks wherever you see. Also, skip this section and head over to the <a href="#analysis">analysis</a> section right away.</p>
<p>If any of you want to repeat it for your personal chats, this is the method I followed:</p>
<ol style="list-style-type: decimal">
<li>Go to the chat window that you want to analyze on whatsapp.</li>
<li>Go to options and there will be an option to export the chat as email.</li>
<li>Send it to your email and download the chat text file</li>
</ol>
<p>I am using whatsapp from an Android phone and I followed the above steps to get the chat text file. Naturally, I searched for preexisting R packages to parse the text file. There were a few packages but they were outdated and incompatible with the current format of the export file. So I wrote my own parser.</p>
<div id="patterns" class="section level3">
<h3>Patterns</h3>
<p>I am basically constructing pattern matchers using regular expressions but I am using an excellent package called rebus that generates the regex patterns using human readable functions. I am pretty sure the patterns I have generated wonâ€™t cover all the possiblities and so might not work for you but with a little tweaking, it can easily be made to.</p>
<pre class="r"><code>pacman::p_load(tidyverse, rebus, lubridate, tidytext, magrittr,
               sentimentr, magrittr, plotly, rtweet, ggthemes)

df_emojis &lt;- rtweet::emojis

pattern_emojis &lt;- df_emojis %&gt;% pull(code) %&gt;%  literal() %&gt;% or1()

pattern_new_message &lt;- 
  NEWLINE %R% one_or_more(DGT) %R% &quot;/&quot; %R% one_or_more(DGT) %R% &quot;/&quot; %R% 
  one_or_more(DGT) %R% &quot;,&quot; %R%
  SPACE %R% one_or_more(DGT) %R% &quot;:&quot; %R% one_or_more(DGT) %R% 
  rebus::or(&quot; AM&quot;, &quot; PM&quot;) %R% &quot; - &quot;

pattern_phone &lt;- 
  literal(&quot;+&quot;) %R% 
  one_or_more(DGT) %R% 
  SPACE %R%
  one_or_more(DGT) %R%
  SPACE %R%
  one_or_more(DGT)

pattern_name &lt;- 
  one_or_more(ALPHA) %R%
  zero_or_more(SPACE) %R%
  zero_or_more(ALPHA) %R%
  zero_or_more(SPACE) %R%
  zero_or_more(ALPHA)</code></pre>
</div>
<div id="parsing" class="section level3">
<h3>Parsing</h3>
<p>After generating the patterns, I executed the below code block to parse the file. (Actually, I ran this and a few more commands to anonymize people.)</p>
<pre class="r"><code># input file
txt_chat &lt;- read_file(&quot;~/projects/sandbox/data/xxx.txt&quot;)

txt_split &lt;- 
  txt_chat %&gt;% 
  str_split(pattern_new_message) %&gt;% 
  pluck(1)

txt_split &lt;- txt_split[2:length(txt_split)]

txt_new_message_pattern &lt;- 
  txt_chat %&gt;% 
  str_extract_all(pattern_new_message) %&gt;% 
  pluck(1)


df_chats &lt;- 
  tibble(chat_time = txt_new_message_pattern,
         chat_text = txt_split) %&gt;%
  mutate(sender = 
           chat_text %&gt;% 
           str_extract(rebus::or(pattern_phone, pattern_name)),
         text = 
           chat_text %&gt;% 
           str_replace(literal(sender), &quot;&quot;) %&gt;% 
           str_replace(zero_or_more(literal(&quot;:&quot;)), &quot;&quot;) %&gt;% 
           str_replace(SPACE, &quot;&quot;) %&gt;% 
           str_trim(),
         time = mdy_hm(chat_time), 
         emoji = str_extract(text, one_or_more(pattern_emojis)),
         emoji_count = str_count(text, pattern_emojis),
         media = text %&gt;% str_detect(literal(&quot;&lt;Media omitted&gt;&quot;)),
         text = ifelse(media, &quot;&quot;, text)
         ) %&gt;%
  add_count(sender) %&gt;%
  filter(!(sender %&gt;% str_detect(DGT)),
         !(sender %&gt;% str_detect(or1(c(&quot;created&quot;, &quot;added&quot;, &quot;left&quot;, &quot;changed&quot;))))) %&gt;%
  mutate(id_chat = row_number()) %&gt;% 
  select(id_chat, time, sender, text, media,  emoji, emoji_count)</code></pre>
<p>Letâ€™s see how the table looks.</p>
<pre><code>## # A tibble: 7,604 x 7
##    id_chat time                sender text         media emoji emoji_count
##      &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt;        &lt;lgl&gt; &lt;chr&gt;       &lt;int&gt;
##  1       1 2018-05-07 23:45:00 A      My life ğŸ˜‚ğŸ˜‚ğŸ˜‚â€¦ FALSE ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚â€¦           5
##  2       2 2018-05-07 23:47:00 A      Sorry for sâ€¦ FALSE ğŸ˜‚ğŸ˜‚            2
##  3       3 2018-05-07 23:47:00 A      It just came FALSE &lt;NA&gt;            0
##  4       4 2018-05-07 23:48:00 B      Wait         FALSE &lt;NA&gt;            0
##  5       5 2018-05-07 23:48:00 B      One bad jokâ€¦ FALSE &lt;NA&gt;            0
##  6       6 2018-05-07 23:49:00 B      Poor network FALSE &lt;NA&gt;            0
##  7       7 2018-05-07 23:49:00 B      &quot;&quot;           TRUE  &lt;NA&gt;            0
##  8       8 2018-05-07 23:50:00 A      ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚   FALSE ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚â€¦           5
##  9       9 2018-05-07 23:52:00 B      All pls to â€¦ FALSE ğŸ™              1
## 10      10 2018-05-07 23:53:00 A      When guys sâ€¦ FALSE &lt;NA&gt;            0
## # ... with 7,594 more rows</code></pre>
<p>Awesome. Now that we have data in a tidy format, letâ€™s delve into the analysis.</p>
</div>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<div id="pet-words" class="section level3">
<h3>Pet words</h3>
<p>What words do the group members use a lot overall?</p>
<pre class="r"><code>df_words &lt;- 
  df_chats %&gt;% 
  unnest_tokens(word, text) %&gt;% 
  anti_join(stop_words, &quot;word&quot;) %&gt;% 
  filter(!word %in% var_ignore_words) 

df_words %&gt;% 
  count(word, sort = T) %&gt;% 
  top_n(15, n) %&gt;% 
  mutate(word = fct_inorder(word) %&gt;% fct_rev()) %&gt;% 
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  theme_minimal()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The members seem to agree. A lot.</p>
<p>If youâ€™re wondering how words like â€œandâ€ and â€œtheâ€ are not the most common, kudos. Those words are indeed the most common but I have removed such words (commonly called â€œstop wordsâ€ in text mining) because they are not informative.</p>
<p>And what words do each of the members use? Letâ€™s look at the five most used words.</p>
<pre class="r"><code>df_words %&gt;% 
  count(sender, word, sort = T) %&gt;% 
  group_by(sender) %&gt;% 
  arrange(desc(n), .by_group = T) %&gt;% 
  slice(1:5) %&gt;% 
  ungroup() %&gt;% 
  mutate(word = word %&gt;% fct_inorder() %&gt;% fct_rev()) %&gt;% 
  ggplot(aes(word, n, fill = sender)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  facet_grid(sender ~ ., scales=&quot;free&quot;) +
  theme(strip.text.x = element_text(angle = 0)) +
  scale_fill_tableau()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Well, most of the words are commonly used ones. Very predictable and not very informative. How do we find words that are important relative to each member? Enter tf-idf! Tf-idf is a concept used in information retrieval where in the importance of each term is measured relative to the prevalence of that term. Basically, if you are using a term a lot and that term is used a lot in general, the count does not accurately reflect who you are. Itâ€™s the words that you use a lot but others donâ€™t that say more about you. Thatâ€™s a very crude explanation but the Wikipedia article is pretty good and simple if you want to dig deeper.</p>
<pre class="r"><code>df_words %&gt;% 
  count(sender, word) %&gt;% 
  filter(n &gt; 15) %&gt;% 
  bind_tf_idf(word, sender, n) %&gt;% 
  group_by(sender) %&gt;% 
  arrange(desc(tf_idf)) %&gt;% 
  slice(1:5) %&gt;% 
  ungroup() %&gt;% 
  mutate(word = word %&gt;% fct_inorder() %&gt;% fct_rev()) %&gt;% 
  ggplot(aes(word, tf_idf, fill = sender)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  facet_grid(sender ~ ., scales=&quot;free&quot;) +
  theme(strip.text.x = element_text(angle = 0)) +
  scale_fill_tableau()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>I feel very foggy personalities seem to emerge (A - playful?, B - philosophical?, C - jejune?). Can actually look at more words but donâ€™t want to give too much away. ğŸ˜œ</p>
</div>
<div id="section" class="section level3">
<h3>âŒš 4âƒ£ ğŸ˜‚ğŸ˜‹ğŸ˜</h3>
<p>In case you are wondering what the heading means, itâ€™s â€œTime for emojisâ€. I promise Iâ€™ll try not to do that again. Anyway, can we repeat the same analysis that we did with words for emojis?</p>
<p>Most used emojis in group:</p>
<pre class="r"><code>df_emoji_split &lt;- 
  df_chats %&gt;% 
  group_by(sender) %&gt;% 
  summarise(emoji = emoji %&gt;% ifelse(is.na(.), &quot;&quot;, .) %&gt;% paste0(collapse = &quot;&quot;),
            emoji_count = sum(emoji_count, na.rm = T)) %&gt;% 
  mutate(emoji_split = 
           map(emoji, function(emo){
             emo &lt;- emo %&gt;% str_split(&quot;&quot;, simplify = T) %&gt;% .[1,]
             tibble(emoji = emo) %&gt;% 
             left_join(df_emojis, by = c(&quot;emoji&quot; = &quot;code&quot;))
           })) %&gt;% 
  select(sender, emoji_split) %&gt;% 
  unnest()

df_emoji_split %&gt;% 
  add_count(emoji, sort = T) %&gt;% 
  mutate(rank = dense_rank(desc(n))) %&gt;% 
  filter(rank &lt;= 10) %&gt;% 
  arrange(desc(n)) %&gt;% 
  mutate(emoji = fct_inorder(emoji) %&gt;% fct_rev()) %&gt;% 
  ggplot(aes(emoji, fill = sender)) +
  geom_bar() +
  coord_flip() +
  theme_minimal() +
  theme(axis.text = element_text(size = 20)) +
  scale_fill_tableau()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>My personal hypothesis was right - A has a serious case of roflitis.</p>
<p>Now letâ€™s put an emoji on the face using tf-idf. These emojis characterize the members better than the count.</p>
<pre class="r"><code>df_emoji_split %&gt;% 
  count(sender, emoji,description, sort = T) %&gt;%
  bind_tf_idf(emoji, sender, n = n) %&gt;% 
  group_by(sender) %&gt;% 
  filter(tf_idf == max(tf_idf)) %&gt;% 
  arrange(sender) %&gt;%
  ungroup() %&gt;% 
  mutate(sender = sender %&gt;% fct_inorder() ) %&gt;% 
  ggplot(aes(sender, y = 10, label = emoji)) +
  geom_text(size = 30) +
  theme_minimal() +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_text(size = 40),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) </code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="sentiment-sentience" class="section level3">
<h3>Sentiment sentience</h3>
<p>Fianlly letâ€™s take a look at the chats from a sentiment perspective. Sentiment analysis is the process of analyzing a given corpus of text and then using some method to analyze sentiment scores. Naively, a score is assigned to a word and the scores of all words in a sentence are added. But a sentence like <em>â€œThe movie was not good at allâ€</em> would still be positively evaluated by such a technique. So I am using a package called <code>sentimentr</code> which accounts for valence shifters (like â€˜notâ€™) and intensifiers (like â€˜veryâ€™). For example,</p>
<pre><code>tibble(
  text = c(&quot;The movie was horrible&quot;,
            &quot;The movie was not good at all&quot;,
            &quot;The movie was okay&quot;,
            &quot;The movie was good&quot;,
            &quot;The movie was very impressive&quot;)) %&gt;% 
   mutate(num = row_number(),
          review = get_sentences(text)) %$% 
   sentiment_by(review, list(num, text)) %&gt;%
   sentimentr::highlight()</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/sentiment.png" /></p>
<p>Now that we know what we can do, letâ€™s calculate the overall sentiment scores</p>
<pre class="r"><code>df_sentiment &lt;- 
  df_chats %&gt;% 
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_sent = get_sentences(sentence),
         id_sentence = row_number())


df_sentiment &lt;- 
  df_sentiment %$%
  sentiment_by(sentence_sent, list(id_sentence, sender)) %&gt;% 
  left_join(df_sentiment) %&gt;% 
  as_tibble()</code></pre>
<pre><code>## Joining, by = c(&quot;id_sentence&quot;, &quot;sender&quot;)</code></pre>
<pre class="r"><code>df_sentiment %&gt;% 
  mutate(week = week(time),
         sentiment = ave_sentiment) %&gt;% 
  filter(sentiment != 0) %&gt;%
  ggplot(aes(sentiment, fill = sender)) +
  geom_density() +
  facet_grid(sender~.)</code></pre>
<p><img src="/analyses/2018-10-03_whatsapp_analysis_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Admittedly, one group-chat tells only a part of the story. If there were a way to get all whatsapp chats in a single step, we could do much richer analyses. For instance, chat behavior of the same person in different groups would be intersting to see. Similarly, it could be seen how the proportion of texts sent to each group or person varies across time. That could shed light into which bonds are becoming stronger and which ones are becoming more weaker. Sentiment analysis on all our texts can help us understand how our moods vary. There are lots of possibilities..</p>
</div>
